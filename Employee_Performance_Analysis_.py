# -*- coding: utf-8 -*-
"""IABAC_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aOS2Kfy8oExkt4trKniORyT2EnY0PYak

**Employee Performance Analysis - INX Future Inc**

Name                    : Jaya Dev Sahoo

E-Mail                  : jayadev.sahoo36@gmail.com

REP Name                : DataMites Solutions Pvt Ltd

Exam country            : India

Assesment ID            : E10901-PR2-V18

Module                  : Certified Data Scientist - Project

Exam Format             : Open Project

Venue Name              : Open Project- IABACâ„¢ Project Submission

Project Assessment      : IABAC

Registered Trainer      : Ashok Kumar A

Submission Deadline Date: 03-Oct-2023

**Business Problem**

The business problem is a binary classification problem. The goal is to predict performance Rating on the Basis of different Features.

**Import Basic Python Packages**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""**Importing Raw Data**"""

df = pd.read_excel('INX_Future_Inc_Employee_Performance_CDS_Project2_Data_V1.8.xls')
df1=pd.DataFrame(df)

df1.head()

"""**Exploratory Data Analysis**"""

df1.shape

df1.info()

df1.isnull().sum()      #Checking Null value Availability

df1.columns

df1.describe()

"""**Handling Duplicates**"""

df1.duplicated().sum()

"""**Domain Analysis:**

Here we have a dataset with 1200 records and 28 features (including the PerformanceRating)


*   **EmpNumber:** Unique employee number for identification.

*   **Age:** Age of the employee.

*   **Gender:**Gender of the employee (Male/Female).

*   **EducationBackground:** Educational background of the employee.

*   **MaritalStatus:** Marital status of the employee (Married/Single/Divorced).

*   **EmpDepartment:** Department in which the employee works.

*   **EmpJobRole:** Job role of the employee.

*   **BusinessTravelFrequency:** Frequency of business travel (Travel_Frequently/Travel_Rarely/Non-Travel).

*   **DistanceFromHome:** Distance from home to workplace.

*   **EmpEducationLevel:** Level of education of the employee.

*   **EmpEnvironmentSatisfaction:** Satisfaction level with the work environment.

*   **EmpHourlyRate:** Hourly rate of the employee.

*  **EmpJobInvolvement:** Level of involvement in the job.

*  **EmpJobLevel:** Job level of the employee.

*   **EmpJobSatisfaction:** Satisfaction level with the job.

*   **NumCompaniesWorked:** Number of companies the employee has worked for.

*   **OverTime:** Whether the employee works overtime (Yes/No).

*   **EmpLastSalaryHikePercent:** Percentage of the last salary hike.

*   **EmpRelationshipSatisfaction:** Satisfaction level with work relationships.

*   **TotalWorkExperienceInYears**: Total work experience of the employee in years.

*   **TrainingTimesLastYear:** Number of training sessions attended last year.

*   **EmpWorkLifeBalance:** Work-life balance satisfaction level.

*   **ExperienceYearsAtThisCompany:** Number of years the employee has worked at this company.

*   **ExperienceYearsInCurrentRole:** Number of years the employee has worked in the current role.

*   **YearsSinceLastPromotion:** Number of years since the last promotion.

*   **YearsWithCurrManager:** Number of years with the current manager.

*   **Attrition:** Whether the employee has left the company (Yes/No).

*   **PerformanceRating:** Performance rating of the employee.

**Visualization**

#Univariate Analysis

## 1.Age
"""

plt.figure(figsize=(10,7))
sns.histplot(x='Age',data=df1)
plt.xlabel('Age',fontsize=5)
plt.show()

"""**Observation**:

The range of age between 18 to 60, most of the employee age between 25 to 40.

**2.Employee Hourly Rate**
"""

plt.figure(figsize=(8,4))
sns.histplot(x='EmpHourlyRate',data=df1)
plt.xlabel('EmpHourlyRate',fontsize=10)
plt.show()

"""**Observation**

The range of employee hourly rate between 30 to 100, most of the employee hourly rate is 45.

**3.Total Work Experience In year**
"""

plt.figure(figsize=(8,4))
sns.histplot(x='TotalWorkExperienceInYears',data=df1)
plt.xlabel('TotalWorkExperienceInYears',fontsize=10)
plt.show()

"""**Observation:** Work experiance range between 0 to 40,Most of the employee experiance ranges between 5 to 10.

**4.Experience Years At This Company**
"""

plt.figure(figsize=(10,7))
sns.histplot(x='ExperienceYearsAtThisCompany',data=df1)
plt.xlabel('ExperienceYearsAtThisCompany',fontsize=20)
plt.show()

"""**Observation:** The Range of experiance in same comapny is 0 to 40,most of the employee join between 0 to 5.

**Use subplot to plot multiple featur**
"""

count = df1[['Gender', 'EducationBackground', 'MaritalStatus','BusinessTravelFrequency','DistanceFromHome',
              'EmpEducationLevel', 'EmpEnvironmentSatisfaction','EmpJobInvolvement', 'EmpJobLevel',
              'EmpJobSatisfaction', 'NumCompaniesWorked', 'OverTime']]
plt.figure(figsize=(15,20))
plotno = 1

for column in count:
    if plotno<=13:
        plt.subplot(4,3,plotno)
        sns.countplot(x=count[column])
        plt.xlabel(column,fontsize=15)
    plotno+=1
plt.tight_layout()
plt.show()

"""**Observation:**

**5.Gender:**

*   Most of the male present in the comapany.

**6.Education Background:**

*   most of the employee education background is Life science
and medical, as well as marketing eduaction background employee is more than 100.
*   other employee Specializatopn eduacation background  is less than 100.

**7.MaritalStatus:**

*   Most of the employee are married , less than 400 employee are single.
*   within 250-300 employee mariatal status are divorced

**8.Business Travel Frequency**:

*   Maximum no of employee travel rarely  for company buisness purpose,and within 200-220 employee travel frequently
*   Within  110-130 employee are  not travell for buisness purpose

**9.Distance From Home:**

*   Distance between home to comapny is 1 to 29.
*   Maximum no of employee home to company distance is less than 10.

**10.Educational Level:**

*   Total range of educational level is 5,Maximum no of employee education level is 3.
*  Education level "2"employee is less than 250 ,"4" is 300-350 and  remianing "1" & "5" education level employee is less than 150.

**11.Employee Environment Satisfaction:**

*   Most of the employee Statisfaction is 3 & 4
*   1 & 2 enviroment satisfaction employee less than 250

**12.Employee Job Involvement:**

*   The majority of employees exhibit a job involvement score of 3, while those with a job involvement score of 2 number fewer than 300.
*   Employees with job involvement scores of 1 and 4 collectively total less than 100, indicating a smaller proportion in these categories.

**13.Employee Job Level:**

*   The range of employee job kevel is 1 to 5,most of the employee job level is 1 & 2.
*   Minimum no of job level is 5, as well as 3 & 4 job level employee is less than 180.

**14.Employee Job Satisfaction:**

*   Most of the employee job satisfaction is 3 & 4 score as well as 1 & 2 score jon satisfaction employee is less than 250.

**15.Number Companies Worked:**

*   The highest number of employees work with a single company.
*   Few employees have experience working in more than five companies, indicating a smaller percentage in this category.

**16.Overtime:**

*  The majority of employees participate in overtime, with fewer than 350 employees engaging in overtime within the company.
"""

count2 = df1[['EmpLastSalaryHikePercent', 'EmpRelationshipSatisfaction','TrainingTimesLastYear','EmpWorkLifeBalance',
               'ExperienceYearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager', 'Attrition',
               'PerformanceRating']]
plt.figure(figsize=(15,20))
plotno = 1

for column in count2:
    if plotno<=10:
        plt.subplot(3,3,plotno)
        sns.countplot(x=count2[column])
        plt.xlabel(column,fontsize=15)
    plotno+=1
plt.tight_layout()
plt.show()

"""**17.Employee Last Salary Hike Percent:**

*   The highest number of employees received a salary hike percentage between 11% to 14%, with the overall range of salary hikes falling between 11% to 25%.

*  Less than 80 employees experienced a salary hike between 15% to 19%

**18.Employee Relationship Satisfaction**

*  The employee relationship satisfaction spans from 1 to 4, with a predominant number of employees reporting satisfaction levels of 3 and 4. Additionally, fewer than 250 employees expressed satisfaction levels of 1 and 2 in their workplace relationships.

**19.Training Times LastYear:**

*   Training time in the last year ranges from 0 to 6. The majority of employees have undergone training for the 2nd and 3rd time, with all other instances of training being less than 100 across the board.

**20.Employee Work Life Balance:**

*   The majority of employees report a work-life balance score of 3, while less than 300 employees have a work-life balance score of 2.
*   A minimum work-life balance score of 1 is recorded, with fewer than 100 employees having a score of 4.

**21.Experiance In Current Role:**

*   The highest level of experience in the current role among employees is 18 years.
*   The majority of employees possess 2 years of experience in their current roles, with those having 7 years of experience numbering fewer than 180.
*   Less than 200 employees do not have any experience in their current roles.

**22.Years Since Last Promotion:**

*   The duration since the last promotion spans from 0 to 15 years.

*   The majority of employees have not been promoted in the last year, with fewer than 300 employees having received a promotion of at least 1 year.

**23.Years With Current Manager:**

*   The majority of employees have worked for 2 years with their current manager, and the minimum duration of employment with a manager is 16 years.

*   The range of experience with the current manager varies from 0 to 17 years, with fewer than 180 employees having 7 years of experience with their current manager.

**24.Attrition:**

*   The majority of employees have not left the company, and fewer than 200 employees are planning to leave.

**25.Performance Rating:**

*   The performance rating ranges from 2 to 4, with the majority of employees having a rating of 3, and fewer employees receiving a rating of 4.
*   Less than 200 employees have a performance rating of 2.

**26.Employee Department**
"""

plt.figure(figsize=(15,10))
sns.countplot(x='EmpDepartment',data=df1)
plt.xlabel('EmpDepartment',fontsize=10)
plt.show()

"""**Observation:**

*   The majority of employees are from the Sales department, followed closely by the Development and Research & Development departments, each with less than 320 employees.

*   The number of employees in the Human Resources and Finance departments is nearly equal.

*   The Data Science department has the fewest employees in the company.

**27.Employee Job Role**
"""

plt.figure(figsize=(20,10))
sns.countplot(x='EmpJobRole',data=df1)
plt.xticks(rotation='vertical')
plt.xlabel('EmpJobRole',fontsize=20)
plt.show()

"""**Observation:**

*   The Sales Executive job role has the highest employee count, and there are exactly 230 employees in the Developer job role.
*   The employee count for Manager R&D and Research Scientist roles is less than 100, while all other job roles have fewer than 65 employees.










.

**Biavariate Analysis**
"""

data = df1[['Age', 'Gender', 'EducationBackground', 'MaritalStatus',
       'EmpDepartment', 'EmpJobRole', 'BusinessTravelFrequency',
       'DistanceFromHome', 'EmpEducationLevel', 'EmpEnvironmentSatisfaction',
       'EmpHourlyRate', 'EmpJobInvolvement', 'EmpJobLevel',
       'EmpJobSatisfaction', 'NumCompaniesWorked', 'OverTime',
       'EmpLastSalaryHikePercent', 'EmpRelationshipSatisfaction',
       'TotalWorkExperienceInYears', 'TrainingTimesLastYear',
       'EmpWorkLifeBalance', 'ExperienceYearsAtThisCompany',
       'ExperienceYearsInCurrentRole', 'YearsSinceLastPromotion',
       'YearsWithCurrManager', 'Attrition']]
plt.figure(figsize=(20,22))
plotno = 1

for column in data:
    if plotno<=27:
        plt.subplot(11,3,plotno)
        sns.countplot(x=data[column],hue=df1.PerformanceRating)
        plt.xlabel(column,fontsize=20)
        plt.ylabel('PerformanceRating')
    plotno+=1
plt.tight_layout()
plt.show()

"""**Observations**

**Age VS performance rating**

*   The majority of employees receive a performance rating of 3, with employees aged between 34-35 receiving the highest number of these ratings.

*   The maximum number of employees receive a rating of 4 at the age of 36.

*   At the age of 60, the highest number of employees receive a performance rating of 2.

**Gender VS performance rating**
*   The highest number of males and females are rated with a performance rating of 3.
*  The number of employees, both male and female, receiving a performance rating of 4 is relatively low.

**EducationBackground VS performance rating**


*   Employees with a background in Life Sciences and Medical education are most frequently rated with a performance rating of 3.

*   Employees with backgrounds in Life Sciences and Medical education are also commonly rated with 2 and 4 performance ratings.

**MaritalStatus VS performance rating**


*  Married employees are most commonly rated with a performance rating of 3, and the same holds true for employees with single or divorced marital statuses.

*   Among married employees, there are more instances of a performance rating of 2 compared to single or divorced employees.

**EmpDepartment VS Perfomance rating**
"""

plt.figure(figsize=(15,7))
sns.countplot(x='EmpDepartment',hue=df1.PerformanceRating,data=df1)
plt.xlabel('EmpDepartment',fontsize=20)
plt.ylabel('PerformanceRating',fontsize=15)
plt.show()

"""*  Employees in the Development, Sales, and Research and Development departments most commonly receive a performance rating of 3.
*   Employees in the Data Science department generally receive lower performance ratings.
*   In the Sales and Research and Development departments, employees with a performance rating of 2 are also quite common.

**Employee job role VS Performance rating**
"""

plt.figure(figsize=(20,10))
sns.countplot(x='EmpJobRole',hue=df1.PerformanceRating,data=df1)
plt.xticks(rotation='vertical')
plt.xlabel('EmpJobRole',fontsize=20)
plt.ylabel('PerformanceRating',fontsize=15)
plt.show()

"""*   Employees in roles such as Developers and Sales Executives commonly receive a performance rating of 3.
*   Employees in roles like Technical Architecture and Delivery Manager are rarely rated with 2 and 4 performance ratings.
*   In the Sales Executive role, fewer than 60 employees are rated with a performance rating of 2.
*   Employees in all other roles predominantly receive a performance rating of 3.

**Business Travel Frequency VS performance rating**

*   Employees who rarely travel mostly receive a performance rating of 3.
*   Employees who frequently travel can receive performance ratings of 2, 3, and 4.

**Overtime VS performance rating**


*   Employees without overtime work predominantly receive a performance rating of 3.


*   Employees working overtime consistently receive performance ratings of 2 and 4.

**Attrition VS performance rating**

*   Employees who have not left the company mostly receive a performance rating of 3.
*   Employees with a performance rating of 3 are also leaving the company.

**EmpEducationLevel VS performance rating**


*   Employees with education levels 3, 4, and 2 predominantly receive a performance rating of 3.
*   If the performance rating is 2 or 4, employees from all education levels are fewer than 60.

**EmpEnvironmentSatisfaction VS performance rating**
*   Most employees with environment satisfaction ratings of 3 and 4 receive a performance rating of 3.
*   When employees have environment satisfaction ratings of 1 or 2, the majority of them have a performance rating of 2.
*  As environment satisfaction ratings increase, the number of employees with a performance rating of 4 also increases.

**EmpJobInvolvement VS performance rating**
*   Employees with job involvement levels of 2 and 3 mostly receive a performance rating of 3.
*  Employees with job involvement levels of 2 and 4 have fewer instances of receiving any performance rating.

**EmpJobLevel VS performance rating**
*   Employees with job levels 1, 2, and 3 predominantly receive a performance rating of 3.
*   Across all job levels, the number of employees receiving performance ratings of 2 and 4 is fewer than 60.

**EmpJobSatisfaction VS performance rating**
*   Employees who rate their job satisfaction as 4 or 3 primarily receive a performance rating of 3 most of the time.

*   Among employees with job satisfaction ratings of 2 and 4, the number of employees receiving a performance rating is less than 55.

**EmpWorkLifeBalance VS performance rating**
*   When employees rate their work-life balance as 3, the majority of them receive a performance rating of 3.
*   If the work-life balance is rated as 1, no employee receives a performance rating of 4.

**Drop the Unnecessary Column**
"""

df1.drop("EmpNumber",inplace=True,axis=1)

df1.head()

"""**Feature Selection**


*  Many predictor variables are present in the dataset. To identify the important ones, correlation coefficients are calculated. These significant variables are then utilized in training models. Through this process, key factors influencing performance are determined, including Department, Job Role, Environment Satisfaction, Last Salary Hike Percent, Work-Life Balance, Experience Years At This Company, Experience Years In Current Role, Years Since Last Promotion, and Years With Current Manager. These features were chosen because their correlation coefficient with Performance Rating exceeded 0.1.
*   Feature transformation involved Standardization and Label Encoding techniques.
*   A comprehensive analysis involving all predictors was conducted. However, this approach led to a decrease in accuracy. Similarly, implementing Principal Component Analysis also resulted in reduced accuracy.


"""

df1.corr()

plt.figure(figsize=(10,8))
sns.heatmap(df1.corr(), annot=True, fmt='0.3f', center=0,linewidths=.5)

"""**Observation**

The correlation matrix provides insights into the relationships between different factors and performance ratings:

*   Strong Positive Correlations with Performance: Factors such as total work experience, employee job level, and employee age exhibit robust positive correlations with performance ratings.
*   Strong Negative Correlation with Performance: Employee environment satisfaction is significantly negatively correlated with performance ratings.
*   Moderate Positive Correlations: Factors like employee last salary hike percent, experience years, and years with the current manager demonstrate moderate positive correlations with performance.
*   Weak Correlations: Employee education level and hourly rate display relatively weak correlations with performance.

In summary, age, total work experience, job level, and environment satisfaction have the most substantial impact on performance ratings. Additionally, employee last salary hike percent and experience years also influence performance, although to a lesser extent. Factors like hourly rate and education level have minimal impact on performance ratings.

**Convert categorical variables into numerical format**
"""

lc=LabelEncoder()
df1.Gender=lc.fit_transform(df1.Gender)
df1.EducationBackground=lc.fit_transform(df1.EducationBackground)
df1.MaritalStatus=lc.fit_transform(df1.MaritalStatus)
df1.EmpDepartment=lc.fit_transform(df1.EmpDepartment)
df1.EmpJobRole=lc.fit_transform(df1.EmpJobRole)
df1.BusinessTravelFrequency	=lc.fit_transform(df1.BusinessTravelFrequency)
df1.OverTime	=lc.fit_transform(df1.OverTime)
df1.Attrition=lc.fit_transform(df1.Attrition)
df1.PerformanceRating=lc.fit_transform(df1.PerformanceRating)

df1.head()

"""**Divided data into Independent and dependent**"""

x=df1.drop('PerformanceRating',axis=1)   #defing Independent variables as x
y=df1['PerformanceRating']               #defing Dependent variables as y

x.head()

"""**Correlation with target variable**

In essence, correlation helps us understand how changes in one variable might correspond to changes in another variable. A positive correlation means that when one variable increases, the other variable tends to increase as well, and vice versa. Conversely, a negative correlation suggests that as one variable increases, the other variable tends to decrease, and vice versa. It's a valuable statistical tool for understanding relationships between different factors in various fields of study.
"""

plt.style.use('ggplot')
ax = x.corrwith(y).plot(kind='barh', grid=True, figsize=(12,12),color='Black')
plt.title("Correlation with target variable\n", size=20,color="red")
plt.xticks(size=15)
plt.yticks(size=15)
ax.xaxis.label.set_color('Blue')
ax.yaxis.label.set_color('brown')
plt.show()

"""Here eight variables are positively correlated with performance rating.

*   Employee Environment Satisfaction
*   Employee Last Salary Hike Percent
*   Education Background
*   Employee Work Life Balance
*   Marital Status
*   Number Companies Worked
*   Employees Education Level

**Checking for Imbalance data**
"""

y.value_counts(normalize=True)*100

"""From results found data is imbalaanced.

**Converting Imbalance data into balanced data**
"""

from imblearn.over_sampling import RandomOverSampler,SMOTENC,SMOTE
oversampling = RandomOverSampler()
x,y = oversampling.fit_resample(x,y)
x = pd.DataFrame(x)
x.columns =['Age', 'Distance From Home', 'Employee Hourly Rate',
       'Number of Companies Worked', 'Employee Last Salary Hike Percent',
       'Total Work Experience In Years', 'Training Times Last Year',
       'Experience Years At This Company', 'Experience Years In Current Role',
       'Years Since Last Promotion', 'Years With Curr Manager',
       'Employee Education Level', 'Employee Environment Satisfaction',
       'Employee Job Involvement', 'Employee Job Level',
       'Employee Job Satisfaction', 'Employee Relationship Satisfaction',
       'Employee Work Life Balance', 'Gender', 'Education Background',
       'Marital Status', 'Employees Department', 'Employees Job Role',
       'Business Travel Frequency', 'Over Time', 'Attrition']
x

y.value_counts(normalize=True)*100

"""Now data is Balanced

**Dividing data into Training and testing**
"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=.20, random_state=0)

x_train.head()

"""**Scaling Data**

"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test= sc.transform(x_test)

"""**PCA**

"""

from sklearn.decomposition import PCA
pca=PCA(n_components=None)
x_train=pca.fit_transform(x_train)
x_test=pca.transform(x_test)
pca.explained_variance_ratio_



"""**Model Development**

**Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression
model_logr = LogisticRegression()
model_logr.fit(x_train,y_train)

# Predicting the model
y_predict_log = model_logr.predict(x_test)

"""**Evaluation the model**

"""

print(accuracy_score(y_test,y_predict_log)*100,'%')
print(classification_report(y_test,y_predict_log))

"""**Observation**

**Precision**: The ability of the classifier not to label as positive a sample that is negative. Scores: 0.76 (class 2), 0.71 (class 3), 0.87 (class 4).

**Recall:** The ability of the classifier to find all the positive samples. Scores: 0.83 (class 2), 0.70 (class 3), 0.80 (class 4).

**F1-Score:** The harmonic mean of precision and recall. Scores: 0.79 (class 2), 0.70 (class 3), 0.83 (class 4).

**Accuracy:** The proportion of correctly classified instances. Accuracy is 78%.


"""

confusion_matrix(y_test,y_predict_log)

"""**observation**

**Class 1:** 153 true positives (correctly predicted as class 1), 35 false negatives (misclassified as class 2), 13 false negatives (misclassified as class 3).

**Class 2:** 117 true positives, 26 false negatives, 22 false positives (misclassified as class 1), 15 false negatives.

**Class 3:** 138 true positives, 6 false positives, 15 false positives, 26 false negatives.

Checking the Train score
"""

y_train_pred=model_logr.predict(x_train)
y_train_pred
acc_train=accuracy_score(y_train,y_train_pred)
print("Train Accuracy" ,acc_train * 100,'%')

"""Checking the Test score"""

y_test_pred=model_logr.predict(x_test)
y_test_pred
acc_test=accuracy_score(y_test,y_test_pred)
print("Test Accuracy",acc_test * 100, "%")

"""**SVM**"""

from sklearn.svm import SVC
svc=SVC() # base model with default parameters
svc.fit(x_train,y_train)

y_predict_svc = svc.predict(x_test)

"""**Evaluate the model**"""

print(accuracy_score(y_test,y_predict_svc)*100,'%')
print(classification_report(y_test,y_predict_svc))

"""**observation**

**Precision:** High for all classes, indicating a low false positive rate. Particularly high for class 2 and 4 (88% and 95% respectively), and slightly lower but still impressive for class 3 (95%).

**Recall:** Generally strong, with class 2 having near-perfect recall (99%) and class 4 also performing very well (97%). Class 3 has a slightly lower recall at 81% but is still good.

**F1-Score:** Harmonic mean of precision and recall is excellent for all classes, indicating a good balance between precision and recall. Highest for class 4 (96%) and slightly lower for class 3 (87%).

**Accuracy:** Overall accuracy is 92%, indicating the proportion of correctly classified instances.
"""

confusion_matrix(y_test,y_predict_svc)

"""**observation**

**Class 1:** 183 true positives, 24 false negatives, 1 false positive.

**Class 2:** 135 true positives, 2 false positives, 5 false negatives.

**Class 3:** 167 true positives, 8 false negatives, 0 false positives

Checking the Train score
"""

y_train_pred=svc.predict(x_train)
print('The Train accuracy : ',accuracy_score(y_train,y_train_pred)*100,'%')

"""Checking the Test score"""

y_test_pred=svc.predict(x_test)
print('The Test accuracy : ',accuracy_score(y_test,y_test_pred)*100,'%')

"""**Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier #importing decision tree
dt=DecisionTreeClassifier() # object creation for decision tree
dt.fit(x_train,y_train)# training the model

y_predict_dt=dt.predict(x_test)

"""**Evaluate the model**

"""

print(accuracy_score(y_test,y_predict_dt)*100,'%')
print(classification_report(y_test,y_predict_dt))

"""**observation**

**Precision:** High for all classes, indicating a low false positive rate. Class 3 has the highest precision (99%), followed closely by classes 2 (88%) and 4 (90%).

**Recall:** Varied but generally good. Class 2 has excellent recall (99%), class 4 also performs well (100%), while class 3 has slightly lower recall at 74%.

**F1-Score:** Harmonic mean of precision and recall is high for all classes, indicating a good balance between precision and recall. Class 4 has the highest F1-score (95%), followed by classes 2 (93%) and 3 (85%).

Accuracy: Overall accuracy is 91%, showing the proportion of correctly classified instances.
"""

confusion_matrix(y_test,y_predict_dt)

"""**observation**

**Class 1:** 184 true positives, 25 false negatives, 1 false positive.

**Class 2:** 123 true positives, 19 false positives, 25 false negatives.

**Class 3:** 173 true positives, 0 false positives, 0 false negatives.

Checking the Train score
"""

y_train_pred=dt.predict(x_train)
print('The Train accuracy : ',accuracy_score(y_train,y_train_pred)*100,'%')

"""Checking the Test score"""

y_test_pred=dt.predict(x_test)
print('The Test accuracy : ',accuracy_score(y_test,y_test_pred)*100,'%')

"""**Random Forest**

"""

from sklearn.ensemble import RandomForestClassifier
rfclass=RandomForestClassifier(n_estimators=87)
rfclass.fit(x_train,y_train)

y_predict_rf=rfclass.predict(x_test)

"""**Evaluate the model**"""

print(accuracy_score(y_test,y_predict_rf)*100,'%')
print(classification_report(y_test,y_predict_rf))

"""**Observation**

**Precision:** Very high across all classes, indicating very few false positives. Class 3 has the lowest precision (93%), still impressive.

**Recall:** Generally high, indicating the model captures most positive instances. Class 3 has the lowest recall (93%), suggesting a few false negatives.

**F1-Score:** Harmonic mean of precision and recall is very high for all classes, indicating a balanced trade-off between precision and recall.

**Accuracy:** Exceptional at 98%, indicating the proportion of correctly classified instances.
"""

confusion_matrix(y_test,y_predict_rf)

"""**Observation**

**Class 1:** 185 true positives, 0 false negatives, 0 false positives.

**Class 2:** 156 true positives, 1 false positive, 10 false negatives.

**Class 3:** 173 true positives.

Checking the Train score
"""

y_train_pred=rfclass.predict(x_train)
print('The Train accuracy : ',accuracy_score(y_train,y_train_pred)*100,'%')

"""Checking the Test score"""

y_test_pred=rfclass.predict(x_test)
print('The Test accuracy : ',accuracy_score(y_test,y_test_pred)*100,'%')

"""
**XGBoost**"""

from xgboost import XGBClassifier
xgb=XGBClassifier()
xgb.fit(x_train,y_train)

y_predict_xgb=xgb.predict(x_test)

print(accuracy_score(y_test,y_predict_xgb)*100,'%')
print(classification_report(y_test,y_predict_xgb))

"""**Observation**

**Precision:**

**Class 0:** 95% precision, indicating that 95% of the instances predicted as class 0 were correct.
**Class 1:** 99% precision, suggesting a high accuracy of positive predictions for class 1.
**Class 2:** 96% precision, demonstrating the model's accuracy in predicting class 2 instances.

**Recall:**

**Class 0:** 99% recall, indicating that 99% of the actual class 0 instances were correctly predicted.

**Class 1:** 90% recall, indicating that 90% of the actual class 1 instances were correctly predicted.

**Class 2:** 100% recall, suggesting that all actual class 2 instances were correctly predicted.

**F1-Score:**

**Class 0:** 97% F1-score, a balanced measure of precision and recall for class 0.

**Class 1:** 94% F1-score, indicating a good balance between precision and recall for class 1.

**Class 2:** 98% F1-score, demonstrating a high balance between precision and recall for class 2.

**Support:**

**Class 0:** 185 instances in the dataset for class 0.

**Class 1:** 167 instances in the dataset for class 1.

**Class 2:** 173 instances in the dataset for class 2.

**Overall Metrics:**

**Accuracy:** The model achieved an overall accuracy of 97%, indicating that 97% of all predictions were correct.

**Macro Avg (Macro-Averaged Metrics):** The macro-average precision, recall, and F1-score are 97%, 96%, and 96%, respectively. This metric calculates the average performance across all classes, giving equal weight to each class.

**Weighted Avg (Weighted-Averaged Metrics):** The weighted-average precision, recall, and F1-score are 97%, 97%, and 97%, respectively. This metric calculates the average performance across all classes, with each class's contribution weighted by its presence in the dataset.

**Conclusion:**

The model exhibits strong predictive capabilities across all classes, with high precision, recall, and F1-scores. It demonstrates accurate classification across the three classes (0, 1, and 2) and achieves an overall accuracy of 97%. These results indicate a robust and reliable performance in the multiclass classification task.


"""

confusion_matrix(y_test,y_predict_xgb)

"""**observation**

**Class 0:**

**True Positives (TP):** 184 instances were correctly classified as Class 0.

**False Positives (FP):** 1 instance from other classes was misclassified as Class 0.

**False Negatives (FN):** 0 instances of Class 0 were misclassified as other classes.

**Class 1:**

**True Positives (TP):** 150 instances were correctly classified as Class 1.

**False Positives (FP):** 10 instances from other classes were misclassified as Class 1.

**False Negatives (FN):** 7 instances of Class 1 were misclassified as other classes.

**Class 2**:

**True Positives (TP):** 173 instances were correctly classified as Class 2.

**False Positives (FP):** 0 instances from other classes were misclassified as Class 2.

**False Negatives (FN):** 0 instances of Class 2 were misclassified as other classes.

**Interpretation:**

*   The model performs well in distinguishing Class 2, with no false positives or false negatives for this class.

*   Class 0 also has a strong performance, with only 1 false positive.

*   Class 1 has a moderate performance, with 10 false positives and 7 false negatives, indicating some difficulty in distinguishing this class from others.

Checking the Train score
"""

y_train_pred=xgb.predict(x_train)
print('The Train accuracy : ',accuracy_score(y_train,y_train_pred)*100,'%')

"""Checking the Test score"""

y_test_pred=xgb.predict(x_test)
print('The Test accuracy : ',accuracy_score(y_test,y_test_pred)*100,'%')

"""**K-Nearest Neighbor**"""

from sklearn.neighbors import KNeighborsClassifier
model_knn = KNeighborsClassifier(n_neighbors=6,metric='euclidean')
model_knn.fit(x_train,y_train)

# Predicting the model
y_predict_knn = model_knn.predict(x_test)

print(accuracy_score(y_test,y_predict_knn))
print(classification_report(y_test,y_predict_knn))

"""**Observation**

**Class 0:**

Precision: 0.76
Recall: 0.95
F1-Score: 0.84
Support: 185

**Class 1:**

Precision: 0.86
Recall: 0.56
F1-Score: 0.68
Support: 167

**Class 2:**

Precision: 0.83
Recall: 0.90
F1-Score: 0.86
Support: 173
Accuracy: 0.81

**Macro Avg (Macro-Averaging):** The average of precision, recall, and F1-score across all classes. Macro avg values are 0.82 (precision), 0.80 (recall), and 0.79 (F1-score).

**Weighted Avg (Weighted-Averaging):** The weighted average of precision, recall, and F1-score, considering class support. Weighted avg values are 0.82 (precision), 0.81 (recall), and 0.80 (F1-score).

**Interpretation:**


*   For Class 0, the model has good precision (0.76) and excellent recall (0.95), indicating that it correctly identifies most instances of Class 0 with relatively few false positives.

*   For Class 1, precision is higher (0.86) but recall is lower (0.56), suggesting that while it's good at correctly identifying Class 1 instances, it misses some, resulting in a trade-off between precision and recall.


*   For Class 2, both precision (0.83) and recall (0.90) are relatively high, indicating a good balance between precision and recall.

The overall accuracy of the model is 0.81, which is the proportion of correctly classified instances across all classes.

"""

confusion_matrix(y_test,y_predict_knn)

"""**Observation**

**Class 0:**

True Positives (TP): 175
False Positives (FP): 5
False Negatives (FN): 5

**Class 1:**

True Positives (TP): 93
False Positives (FP): 48
False Negatives (FN): 26

**Class 2:**

True Positives (TP): 155
False Positives (FP): 10
False Negatives (FN): 8

**Interpretation:**

Class 0: The model correctly identified 175 instances of Class 0. It incorrectly classified 5 instances of Class 1 as Class 0 (false positives) and missed 5 instances of Class 0 (false negatives).

Class 1: The model correctly identified 93 instances of Class 1. It incorrectly classified 48 instances of Class 0 as Class 1 and missed 26 instances of Class 1.

Class 2: The model correctly identified 155 instances of Class 2. It incorrectly classified 10 instances of Class 0 as Class 2 and missed 8 instances of Class 2.

Checking the Train score
"""

y_train_pred=model_knn.predict(x_train)
print('The Train accuracy : ',accuracy_score(y_train,y_train_pred)*100,'%')

"""Checking the Test score"""

y_test_pred=model_knn.predict(x_test)
print('The Train accuracy : ',accuracy_score(y_test,y_test_pred)*100,'%')

"""**Naive Bayes Bernoulli**"""

from sklearn.naive_bayes import BernoulliNB
model_nb = BernoulliNB()
model_nb.fit(x_train,y_train)

y_predict_nb = model_nb.predict(x_test)

"""**Evaluate the model**"""

print(accuracy_score(y_test,y_predict_nb))
print(classification_report(y_test,y_predict_nb))

"""**Observation**

**Class 0:**

**Precision:** 0.69 (69% of instances predicted as Class 0 were actually Class 0)

**Recall:** 0.61 (61% of actual Class 0 instances were correctly predicted)

**F1-Score:** 0.65 (harmonic mean of precision and recall for Class 0)

**Support:** 185 instances

**Class 1:**

**Precision:** 0.61 (61% of instances predicted as Class 1 were actually Class 1)

**Recall:** 0.65 (65% of actual Class 1 instances were correctly predicted)

**F1-Score:** 0.63 (harmonic mean of precision and recall for Class 1)

**Support:** 167 instances

**Class 2:**

**Precision:** 0.70 (70% of instances predicted as Class 2 were actually Class 2)

**Recall:** 0.75 (75% of actual Class 2 instances were correctly predicted)

**F1-Score:** 0.72 (harmonic mean of precision and recall for Class 2)

**Support:** 173 instances

**Overall Metrics:**

**Accuracy:** 67% (proportion of correctly classified instances across all classes)

**Macro Avg Precision:** 0.67 (average precision across all classes)

**Macro Avg Recall:** 0.67 (average recall across all classes)

**Macro Avg F1-Score:** 0.67 (average F1-score across all classes)

**Weighted Avg Precision:** 0.67 (precision averaged by the number of samples in each class)

**Weighted Avg Recall:** 0.67 (recall averaged by the number of samples in each class)

**Weighted Avg F1-Score**: 0.67 (F1-score averaged by the number of samples in each class)

**Interpretation:**

The model shows moderate performance across all classes, with F1-scores ranging from 0.63 to 0.72. Class 2 (precision: 70%, recall: 75%) performs slightly better than Class 0 (precision: 69%, recall: 61%) and Class 1 (precision: 61%, recall: 65%). The weighted average metrics indicate that the model's performance is balanced across classes, with no significant class imbalance affecting the overall evaluation. The model correctly classifies 67% of the  in the dataset.





"""

confusion_matrix(y_test,y_predict_nb)

"""**Observation**

**Class 0:**

Correctly predicted: 113 instances
Misclassified as Class 1: 40 instances
Misclassified as Class 2: 32 instances

**Class 1:**

Correctly predicted: 108 instances
Misclassified as Class 0: 35 instances
Misclassified as Class 2: 24 instances

**Class 2:**

Correctly predicted: 130 instances
Misclassified as Class 0: 15 instances
Misclassified as Class 1: 28 instances

**Interpretation:**

**Class 0:**

The model has a higher tendency to misclassify Class 0 instances as Class 1 and Class 2.
True Positives (Correctly Predicted Class 0): 113 instances
Misclassifications: 40 instances as Class 1 and 32 instances as Class 2

**Class 1:**

The model has a higher tendency to misclassify Class 1 instances as Class 0.
True Positives (Correctly Predicted Class 1): 108 instances
Misclassifications: 35 instances as Class 0 and 24 instances as Class 2

**Class 2:**

The model has a relatively balanced performance for Class 2.
True Positives (Correctly Predicted Class 2): 130 instances
Misclassifications: 15 instances as Class 0 and 28 instances as Class 1

The model seems to have the most difficulty distinguishing Class 0 and Class 1, with a considerable number of misclassifications between these two classes.

Checking the Train score
"""

y_train_pred=model_nb.predict(x_train)
print('The Train accuracy : ',accuracy_score(y_train,y_train_pred)*100,'%')

"""Checking the Test score"""

y_test_pred=model_nb.predict(x_test)
print('The Train accuracy : ',accuracy_score(y_test,y_test_pred)*100,'%')

"""**Building different Models and validating using 10 fold cross validation**"""

models = []
models.append(('LR', LogisticRegression()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('Decison-Tree', DecisionTreeClassifier()))
models.append(('Bernoulli', BernoulliNB()))
models.append(('SVM', SVC()))
models.append(('RandForest',RandomForestClassifier(max_depth = 8, n_estimators = 120)))
models.append(('XGB', XGBClassifier(n_estimators = 120)))

from sklearn.model_selection import RandomizedSearchCV
from sklearn import model_selection

results = []
names = []
for name, model in models:
    kfold = model_selection.KFold(n_splits=10)
    cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)
    msg = "{}: {}".format(name, cv_results.mean())
    print(msg)

"""**Conclusion**

*   The Random Forest model achieved the highest accuracy at 98%, indicating outstanding performance in classifying instances into their respective classes. It displayed excellent precision, recall, and F1-scores across all classes.

*   XGBoost also performed exceptionally well with an accuracy of 97% and demonstrated a strong balance between precision and recall for all classes.

*   Support Vector Machine (SVM) showed robust performance with an accuracy of 92% and maintained high precision, recall, and F1-scores for all classes.

*   Decision Tree and K-Nearest Neighbors (KNN) models performed reasonably well with accuracies of 91% and 81%, respectively, and balanced precision and recall metrics.

*   Logistic Regression provided a good baseline with 78% accuracy, while Naive Bayes (Bernoulli) showed moderate performance with 67% accuracy.

**Based on these results, the Random Forest, XGBoost, and SVM models are recommended for this classification task due to their high accuracy and balanced performance across all classes.**
"""

